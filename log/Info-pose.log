python train.py --dataset=fashion_mnist --epoch=25 --adversarial_loss_mode=gan
python train.py --dataset=celeba --epoch=25 --adversarial_loss_mode=gan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=dragan
python train.py --dataset=anime --epoch=200 --adversarial_loss_mode=wgan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=line --n_d=5
python train.py --dataset=celeba --epoch=25 --adversarial_loss_mode=gan
python train.py --dataset=celeba --epoch=25 --adversarial_loss_mode=gan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=dragan
python train.py --dataset=celeba --epoch=25 --adversarial_loss_mode=gan --gradient_penalty_mode=lp --gradient_penalty_sample_mode=dragan
python train.py --dataset=celeba --epoch=25 --adversarial_loss_mode=lsgan
python train.py --dataset=celeba --epoch=50 --adversarial_loss_mode=wgan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=line --n_d=5
python train.py --dataset=celeba --epoch=50 --adversarial_loss_mode=wgan --gradient_penalty_mode=lp --gradient_penalty_sample_mode=line --n_d=5

#--------------训练参数---------------

#adversarial_loss_mode: [gan / wgan / lsgan / hinge_v1 / hinge_v2]
#gradient_penalty: []

#-----------pose-----------

python train.py --dataset_name=pose10 --adversarial_loss_mode=gan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=dragan
# 10张图片1张1张的进，有些动作没有训练出来

python train.py --dataset_name=pose10 --adversarial_loss_mode=gan
# 训练比较清晰，但是只是一个动作无多样性

python train.py --dataset_name=pose10 --adversarial_loss_mode=gan --gradient_penalty_mode=lp --gradient_penalty_sample_mode=dragan
# 会出现两个人，但是两个人这种颜色比较浅(可以考虑maxpool)

python train.py --dataset_name=pose10 --epochs=1000 --adversarial_loss_mode=lsgan
#训练在5000多次时较稳定，但只有一个动作,无多样性


#------------wgan----------------

python train.py --dataset_name=pose10 --epochs=1000 --adversarial_loss_mode=wgan
#训练崩溃

python train.py --dataset=pose10 --epoch=1000 --adversarial_loss_mode=wgan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=line --n_d=5
#总样本只有n_d=1的1/4，总体比较模糊

python train.py --dataset=pose10 --epoch=1000 --adversarial_loss_mode=wgan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=line --n_d=2
#动作比较全！，部分动作会演化为双人,双人效果也较多

python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=wgan --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=dragan --n_d=2
#训练收敛比较慢，中间出现过崩溃，后期可以生成但比较模糊，介于模糊和崩溃之间

python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=wgan --gradient_penalty_mode=lp --gradient_penalty_sample_mode=line --batch_size=1 
python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=wgan --gradient_penalty_mode=lp --gradient_penalty_sample_mode=dragan --batch_size=1 
#这一组比line双人更多一些

#--------------------hingev1
python train.py --dataset_name=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=line --batch_size=1 
#动作重复较多
python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=dragan --batch_size=1 
#动作略微不全，但动作完整的较为清晰，动作不完整或者双人的较暗

python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=dragan --batch_size=1 
python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 
#动作较全,双人不清晰

python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=real --batch_size=1 
#动作比上述两个少一些

python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=fake --batch_size=1
#训练出了完全不同的风格，甚至失败 


python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=lp --gradient_penalty_sample_mode=dragan --batch_size=1 
#清晰不全，有个别清晰的两人图19500
python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v1 --gradient_penalty_mode=lp --gradient_penalty_sample_mode=line --batch_size=1
#清晰但是不全 

#--------------------hingev2------------
python train.py --dataset_name=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=none --gradient_penalty_sample_mode=line --batch_size=1 
python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=1-gp --gradient_penalty_sample_mode=dragan --batch_size=1 
#效果好但是动作不全，和v1类似

python train.py --dataset=pose10 --epoch=2000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=dragan --batch_size=1 
#动作比下一个略少，无双人

python train.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-04-losss
#效果最佳，动作全双人清晰

python train.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-04-PG_loss_v4
#1000次ep后loss衰减

python train64.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-05-Pgdown64-05_03

python train128.py --dataset=pose10 --epoch=6000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-04-PG_loss_v5_128

python train32.py --dataset=pose10 --epoch=6000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-04-PG_loss_v5_32

python train.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-04-PG_loss_size_256

python train.py --dataset=pose10 --epoch=12000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-size_256-SGP3
#三段loss目标改变

python train64.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-05_04-SGD

python train64.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-05_04-Adam_v1

python train64.py --dataset=pose10 --epoch=5000 --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-05_04-pg-v2

#----------------------新loss mnist--------------------
python train128.py --dataset=pose10 --epoch=6000 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv1_2
python train128.py --dataset=mnist --epoch=6000 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=64 --experiment_name=hingv1_2_mnist
python train128.py --dataset=mnist --epoch=6000 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=64 --experiment_name=hingv1_2_mnist_origin

#------------------------新文件夹 确认实验 ep=5000--------------
python train_pose.py --adversarial_loss_mode=hinge_v2 --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --experiment_name=hingv2-gp-05_decay

python train_pose.py --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-05-lossD-GP
#距离为05，D的loss衰减:D_loss = 1/(1+0.001*ep)*D_loss，bias=flase

python train_pose.py --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2-gp-05-lossALL-GP
#距离为05，G和D的loss衰减:D_loss = 1/(1+0.001*ep)*D_loss，bias=flase

python train_pose_decay.py --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2gp-05dis-LossWithTarget-GP
#距离为05，G和D的loss衰减: 1/(1+0.01*ep)*loss-1/(1+0.001*ep)*loss，且目标增量也有衰减 (args.epochs-ep)/args.epochs , bias=flase, D比G衰减的慢

#--------------------两级反转--------------
python train_pose_LJFZ.py --gradient_penalty_mode=0-gp --gradient_penalty_sample_mode=line --batch_size=1 --experiment_name=hingv2gp-LJFZ
#一开始样本生成较好，原样本颜色也较深。
#hinge距离设为1，G训练很早就收敛了

